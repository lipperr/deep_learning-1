{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бонусное домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0\n",
    "\n",
    "В данном домашнем задании вам предстоит реализовать СLIP -- self-supervision модель которая выучивает зависимости между картинками и текстов в едином векторном пространстве. Для выполнения этого домашнего задания вам понадобится GPU и несколько дополнительных библиотек. Автор рекомендует делать все исключительно в Kaggle. \n",
    "\n",
    "\n",
    "[Ссылка на датасет](https://www.kaggle.com/datasets/keenwarrior/small-flicker-data-for-image-captioning)\n",
    "\n",
    "[Ссылка на статью](https://openai.com/research/clip)\n",
    "\n",
    "Задания в ноутбуке будут во многом опираться на статью, поэтому рекомендуется ее прочитать перед выполнением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286e7892-d919-4e36-902d-822051fc429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import timm\n",
    "PATH_TO_IMAGES = \n",
    "\n",
    "\n",
    "\n",
    "to_tensor = T.ToTensor()\n",
    "import typing as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (8 баллов)\n",
    "\n",
    "Для начала нам нужно реализовать составляющие модели: Кодировщик картинок, текста и проектор на какое-то маломерное пространство. В папке с заданием есть соответствующие файлы, заполните пропуски в них опираясь на docstring-и.\n",
    "\n",
    "Разбалловка следующая: \n",
    "\n",
    "Правильно реализованные кодировщики: 2 балла.\n",
    "\n",
    "Правильно реализованный проектор: 2 балла.\n",
    "\n",
    "Правильно реализованный класс СLIP: 4 балла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8033dae-8ebf-4638-85fc-648688885d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .CLIPDataset import CLIPDataset\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "from .ImageEncoder import ImageEncoder\n",
    "from .ProjectionHead import ProjectionHead\n",
    "from .TextEncoder import TextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, image_path, image_filenames, captions, tokenizer):\n",
    "        \"\"\"\n",
    "        :image_path -- path to images\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names\n",
    "        :tokenizer -- LM Tokenizer \n",
    "        \"\"\"\n",
    "        self.max_tokenizer_length = 200\n",
    "        self.truncation = True\n",
    "        self.padding = True\n",
    "        self.image_path = image_path\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=self.max_tokenizer_length)\n",
    "        self.transforms = T.Resize([224, 244]) \n",
    "\n",
    "    def __getitem__(self, idx: int) -> tp.Dict[str, tp.Union[torch.Tensor, str]]:\n",
    "\n",
    "        \"\"\"\n",
    "        This one should return dict(keys=['image', 'caption'], value=[Image, Caption])\n",
    "        \"\"\"\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx]) for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "        image = Image.open(self.image_filenames[idx])\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f100e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, model_name=\"resnet50\", pretrained=True, trainable=False\n",
    "        ):\n",
    "        \"\"\"\n",
    "        We will use standard pretrained ResNet50, and set freeze its parameters.\n",
    "        Look the documentation of TIMM on how to donwload the model: https://timm.fast.ai/\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained, num_classes=0)\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58911dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", pretrained=True, trainable=False):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Create the model and set its weights frozen. \n",
    "        Use Transformers library docs to find out how to do this.\n",
    "        \"\"\"\n",
    "        # use the CLS token hidden representation as the sentence's embedding\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Pass the arguments through the model and make sure to return CLS token embedding\n",
    "        \"\"\"\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return out.last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=256,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Here you should write simple 2-layer MLP consisting:\n",
    "        2 Linear layers, GELU activation, Dropout and LayerNorm. \n",
    "        Do not forget to send a skip-connection right after projection and before LayerNorm.\n",
    "        The whole structure should be in the following order:\n",
    "        [Linear, GELU, Linear, Dropout, Skip, LayerNorm]\n",
    "        \"\"\"\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim) #Projection into a small latent space\n",
    "        # Make everything else yourself.\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform forward pass, do not forget about skip-connections.\n",
    "        \"\"\"\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x += projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77152488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "db376447-d24d-4df7-9904-a4db7c02beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, image_embedding=2048, text_embedding=768, temp =1.0):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projections = ProjectionHead(image_embedding)\n",
    "        self.text_projections = ProjectionHead(text_embedding)\n",
    "        self.temp = temp\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        :batch: dict of images and text\n",
    "        Here is what you should do:\n",
    "        1) extract image and text features from batch\n",
    "        2) project features into projection space (small latent space)\n",
    "        3) compute cosine similarity with temperature this will be your logits\n",
    "        4) compute \"true\" logits (eg. cosine similarity between images and images, text and text)\n",
    "        5) create targets by averaging similarities from step above (do not forget about temperature)\n",
    "        6) compute mean loss (see paper)\n",
    "        7) return loss\n",
    "\n",
    "        Overall: read paper.\n",
    "        \n",
    "        \"\"\"\n",
    "        image_features = self.image_encoder(batch['image'])\n",
    "        text_features = self.text_encoder(batch['input_ids'], batch['attention_mask'])\n",
    "\n",
    "        image_projection = self.image_projections(image_features)\n",
    "        text_projection = self.text_projections(text_features)\n",
    "\n",
    "        logits = (text_projection @ image_projection.T) / self.temp\n",
    "\n",
    "        im_similarity = image_projection @ image_projection.T\n",
    "        text_similarity = text_projection @ text_projection.T\n",
    "\n",
    "        targets = nn.functional.softmax((im_similarity + text_similarity) / 2 * self.temp, dim=-1)\n",
    "\n",
    "        image_loss = CE(logits.T, targets.T)\n",
    "        text_loss = CE(logits, targets)\n",
    "        loss = (image_loss + text_loss) / 2.\n",
    "\n",
    "        return loss.mean()\n",
    "    \n",
    "\n",
    "def CE(preds, targets):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    return loss   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. (0 Баллов)\n",
    "\n",
    "Здесь вам нужно вписать правильный путь до csv файла на своей машине и запустить код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "03ac2b6c-f76e-4e9a-967f-da89e5da0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_valid_dfs():\n",
    "    dataframe = pd.read_csv(f\"{PATH_TO_IMAGES}\")\n",
    "    dataframe[\"id\"] = np.array(list(dataframe.index))\n",
    "    max_id = dataframe[\"id\"].max() + 1\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e03c2216-3007-4126-aa39-35aa40c4f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=1,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3812cb05-d764-42e4-8bac-385dbc89e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"CrossEntropyLoss\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "    \n",
    "    def __format__(self, formatspec):\n",
    "        text = f\"{self.name}: {format(self.avg, formatspec)}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6aabd761-efc4-464a-9278-18732667f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", total=len(train_loader)):\n",
    "        batch = {key: value.to(device) for key, value in batch.items() if key != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "        loss_meter.update(loss.item(), batch[\"image\"].shape[0])\n",
    "    return loss_meter\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, validation_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "    for batch in tqdm(validation_loader, desc=\"Validating\", total=len(validation_loader)):\n",
    "        batch = {key: value.to(device) for key, value in batch.items() if key != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        loss_meter.update(loss.item(), batch[\"image\"].shape[0])\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. (2 балла)\n",
    "\n",
    "За вас написан минимальный код для обучения, если он запускается и модель учится, то за этот пункт вы получите 0.5 балла. Чтобы получить полный балл за задание вам нужно будет провести несколько экспериментов и поподбирать гиперпараметры. Можно начать со статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f1de37ff-d443-4238-b15d-3c6ca673d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCH = 10\n",
    "def procedure():\n",
    "    train_df, validation_df = make_train_valid_dfs()\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    train_loader, _ = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "    val_loader, _ = build_loaders(validation_df, tokenizer, mode=\"valid\")\n",
    "    model = CLIP().to(device)\n",
    "    params = [{\"params\": model.image_encoder.parameters()}, \n",
    "              {\"params\" : model.text_encoder.parameters()},\n",
    "              {\"params\" : itertools.chain(model.image_projections.parameters(),\n",
    "                                          model.text_projections.parameters())}]\n",
    "    optimizer = torch.optim.Adam(params)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=1, factor=0.8)\n",
    "    step=\"epoch\"\n",
    "    for epoch in range(EPOCH):\n",
    "        print(f\"Epoch: {epoch}. Train and Validation in progress...\")\n",
    "        model.train()\n",
    "        train_loss = train(model, train_loader, optimizer, lr_scheduler, step)\n",
    "        model.eval()\n",
    "        val_loss = validate(model, val_loader)\n",
    "        \n",
    "        lr_scheduler.step(val_loss.avg)\n",
    "        print(f\"Epoch: {epoch},\", end=\"\\n\")\n",
    "        print(f\"Train loss: {train_loss:0.3f}\", end=\"\\n\")\n",
    "        print(f\"Validation loss: {val_loss:0.3f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe63a271-1f68-4eb9-b77e-3d5d6b98aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = procedure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (0 баллов)\n",
    "\n",
    "Просто посмотрим на результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "92b890cd-9136-4be7-95bb-8c19dd8152ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_image_embeddings(valid_df, model):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    valid_loader, _ = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "    valid_image_embeddings = []\n",
    "    for batch in tqdm(valid_loader, desc=\"Getting embeddings\", total=len(valid_loader)):\n",
    "        batch = {key: value.to(device) for key, value in batch.items() if key != \"caption\"}\n",
    "        image_features = model.image_encoder(batch[\"image\"].permute(0, 3, 1, 2)).to(device)\n",
    "        image_embeddings = model.image_projections(image_features)\n",
    "        valid_image_embeddings.append(image_embeddings)\n",
    "    return torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "10a70109-c5a3-4677-9a3c-f83ad7c0ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "@torch.inference_mode()\n",
    "def find_match(model, image_embeddings, text, image_filenames, num_examples=4):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    text_encoded = tokenizer([text])\n",
    "    batch = {key : torch.tensor(value).to(device) for key, value in text_encoded.items()}\n",
    "    \n",
    "    text_features = model.text_encoder(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "    text_embeddings = model.text_projections(text_features)\n",
    "    \n",
    "    norm_image_embeddings = nn.functional.normalize(image_embeddings, p=2, dim=-1)\n",
    "    norm_text_embeddings = nn.functional.normalize(text_embeddings, p=2, dim=-1)\n",
    "    \n",
    "    similarity = norm_text_embeddings @ norm_image_embeddings.T\n",
    "    \n",
    "    ans, ans_index = torch.topk(similarity.squeeze(0), num_examples * 5)\n",
    "    match = [image_filenames[index] for index in ans_index[::5]]\n",
    "    fig, ax = plt.subplots(int(num_examples/2), int(num_examples/2), figsize= (10, 10))\n",
    "    for m, a in zip(match, ax.flatten()):\n",
    "        image = Image.open(f\"{PATH_TO_IMAGES}\" + f\"/{m}\")\n",
    "        image = image.convert(\"RGB\")\n",
    "        a.imshow(image)\n",
    "        a.axis(\"off\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a465dc-4385-4566-a508-564508c66538",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, valid_df = make_train_valid_dfs()\n",
    "image_embeddings = get_image_embeddings(valid_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f8875f-f705-45d8-8cef-81b11a4c21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_match(model, image_embeddings, \"dogs\", valid_df[\"image\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Опишите свои результаты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
